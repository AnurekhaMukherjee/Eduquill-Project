{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To build a video recommendation algorithm based on user interests and video features, we can follow these steps:\n",
        "\n",
        "1. Data Collection\n",
        "User Data: Create a dummy dataset with fields like watch history, language, and location.\n",
        "Video Data: Use the provided API to fetch video data. We can use pagination to get a comprehensive dataset.\n",
        "2. Data Preprocessing\n",
        "Clean and Normalize Data: Ensure all data is in a consistent format.\n",
        "Feature Extraction: Extract relevant features from both user and video data.\n",
        "3. Algorithm Development\n",
        "Collaborative Filtering: Use user watch history to find similar users and recommend videos they liked.\n",
        "Content-Based Filtering: Recommend videos similar to those the user has watched, based on features like title, comments, and category.\n",
        "Hybrid Approach: Combine both collaborative and content-based filtering for better recommendations.\n",
        "4. Model Training\n",
        "Train Your Model: Use machine learning algorithms like k-nearest neighbors (KNN), matrix factorization, or deep learning models.\n",
        "Evaluation: Split your data into training and testing sets to evaluate the performance of our model.\n",
        "5. Implementation\n",
        "API Integration: Fetch real-time data using the provided API.\n",
        "Recommendation Engine: Implement the recommendation logic in our application.\n",
        "6. Testing and Optimization\n",
        "A/B Testing: Test different versions of our recommendation algorithm to see which performs better.\n",
        "Optimization: Continuously improve our algorithm based on user feedback and performance metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "h4-fOYm4d0NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install implicit"
      ],
      "metadata": {
        "id": "JWoNFbdOZWI-",
        "outputId": "789babe7-50f9-45bd-92d7-b4aa9bc97c85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting implicit\n",
            "  Downloading implicit-0.7.2-cp310-cp310-manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from implicit) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.10/dist-packages (from implicit) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from implicit) (4.66.5)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from implicit) (3.5.0)\n",
            "Downloading implicit-0.7.2-cp310-cp310-manylinux2014_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: implicit\n",
            "Successfully installed implicit-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "id": "GJXBy4WscRP4",
        "outputId": "7a3094b7-094b-477e-a62a-c47ec9d12907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357280 sha256=b8c2d75748d3f375e3849ec43b2bcefae26d190a311b1964f3ae0a5e4bce6579\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset, Reader, KNNBasic\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "# Load data\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "trainset, testset = train_test_split(data, test_size=0.25)\n",
        "\n",
        "# Use KNN algorithm\n",
        "algo = KNNBasic()\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Make predictions\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy.rmse(predictions)\n"
      ],
      "metadata": {
        "id": "E51OkFjIatd9",
        "outputId": "2d4a4d61-6ae0-4043-db9b-1f3137343450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.9794\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.97938012429076"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wor0A6RUas5Q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}